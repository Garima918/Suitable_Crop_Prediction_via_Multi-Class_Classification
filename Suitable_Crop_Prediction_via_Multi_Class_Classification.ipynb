{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9y//IhEU/SL0v27FSlyag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Garima918/Suitable_Crop_Prediction_via_Multi-Class_Classification/blob/main/Suitable_Crop_Prediction_via_Multi_Class_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Loading**"
      ],
      "metadata": {
        "id": "1OLiUmnJUi3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JObaWuzl8W3L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T-mHPmi8Z_8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/Crop_recommendation.csv')\n",
        "# We can import file via local repository/drive or through Dataset librairies like Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Describing Dataset**"
      ],
      "metadata": {
        "id": "Ze_IkHPnU_Fe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQdeFrbt8dgo"
      },
      "outputs": [],
      "source": [
        "print(df.info())\n",
        "print(df.describe())\n",
        "# If we get total entries equal to the entries in all variable types, then we can conclude that there are no missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization**"
      ],
      "metadata": {
        "id": "0os_4pudUvwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TQUZyEU8dpw"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.histplot([df['temperature'], df['humidity'], df['rainfall']], bins= 50, kde=True, alpha=0.8, palette = ['orange','blue','green'])\n",
        "# We can add as many columns we want to visualize and change other paramenters as required."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_g = df.groupby('label')\n",
        "df_g_c = df_g[\"label\"].count()\n",
        "print(df_g_c)\n",
        "sns.barplot(hue = df_g_c.index, y = df_g_c.values)"
      ],
      "metadata": {
        "id": "Wcjd-10i_6JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrWw0c9w8dsu"
      },
      "outputs": [],
      "source": [
        "k = df.select_dtypes(include = 'number').corr()\n",
        "sns.heatmap(k, annot=True, cmap='bwr')\n",
        "#Other colormaps can be - bwr, reds, oranges etc.\n",
        "#Nominal categorical data must be excluded, not encoded, for correlation analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52hAtHQ98dvj"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df, hue='label')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Splitting**"
      ],
      "metadata": {
        "id": "k-i_kLBgVrTd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB7AeUpI8dx1"
      },
      "outputs": [],
      "source": [
        "x = df.drop(columns = 'label', axis=1)\n",
        "y = df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Flt5k3lZnW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 9)\n",
        "pmf = pd.DataFrame()\n",
        "#pmf- performance_metrics_file\n",
        "#Additional dataframe (pmf) created to store all the model's performance score for better visualization among models trained."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "c5j7y4avWc4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Class Imbalance**"
      ],
      "metadata": {
        "id": "4hD7L0OyWjhz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F0MxxswdvCx"
      },
      "outputs": [],
      "source": [
        "#from imblearn.over_sampling import SMOTE\n",
        "#sm = SMOTE(random_state=9)\n",
        "#X, Y = sm.fit_resample(x, y)\n",
        "#from imblearn.over_sampling import RandomOverSampler\n",
        "#ros = RandomOverSampler(random_state=9)\n",
        "#x_train_ros, y_train_ros = ros.fit_resample(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Scaling**"
      ],
      "metadata": {
        "id": "_5z7AJOxXkY8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSqcwfnK8d6q"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)  # fit only on train\n",
        "x_test_scaled = scaler.transform(x_test)    #transform test\n",
        "X_train_df = pd.DataFrame(x_train_scaled, columns=x.columns)\n",
        "X_test_df  = pd.DataFrame(x_test_scaled, columns=x.columns)\n",
        "print(x_train_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance Measures**\n"
      ],
      "metadata": {
        "id": "0FzBjrQhDUuj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzajp3EM8d9K"
      },
      "outputs": [],
      "source": [
        "#Importing modules for Performance Metrics\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "#from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_indicator = ['Precision','Accuracy','Recall','F1 Score']"
      ],
      "metadata": {
        "id": "OBzugfhsqwWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1 = y_test.groupby(y_test).count()\n",
        "print(y1)\n",
        "# We will get to know how many instances of different categories are there in testing dataset.\n",
        "#Series don't have column, hence we need to group by series name in brackets also and use count()."
      ],
      "metadata": {
        "id": "JaiQ8q-BG2AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Logistic Regression Model**"
      ],
      "metadata": {
        "id": "f613ZeEPZFeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJtZH_mZ8d-8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "x_train, x_test, y_train, y_test = x_train.copy(), x_test.copy(), y_train.copy(), y_test.copy()\n",
        "log_r = LogisticRegression()\n",
        "log_r.fit(x_train, y_train)\n",
        "log_r_pred = log_r.predict(x_test)\n",
        "\n",
        "log_r_precision_score = precision_score(y_test, log_r_pred, average = \"weighted\")\n",
        "log_r_accuracy_score = accuracy_score(y_test, log_r_pred)\n",
        "log_r_recall_score = recall_score(y_test, log_r_pred, average = \"weighted\")\n",
        "log_r_f1_score = f1_score(y_test, log_r_pred, average = \"weighted\")\n",
        "\n",
        "pmf[\"Log_R\"] = pd.DataFrame({'Log R': [log_r_precision_score, log_r_accuracy_score, log_r_recall_score, log_r_f1_score]}, index = metrics_indicator)\n",
        "cm = confusion_matrix(y_test, log_r_pred, labels = log_r.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = log_r.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation = 90)\n",
        "plt.yticks(rotation = 0)\n",
        "plt.title('Logistic Regression')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Decision Tree Model**"
      ],
      "metadata": {
        "id": "3V209KlTorsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFj7dE428eBV"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(x_train, y_train)\n",
        "dt_pred = dt.predict(x_test)\n",
        "\n",
        "dt_precision_score = precision_score(y_test, dt_pred, average = \"weighted\")\n",
        "dt_accuracy_score = accuracy_score(y_test, dt_pred)\n",
        "dt_recall_score = recall_score(y_test, dt_pred, average = \"weighted\")\n",
        "dt_f1_score = f1_score(y_test, dt_pred, average = \"weighted\")\n",
        "\n",
        "pmf[\"DT\"] = pd.DataFrame({'DT':[dt_precision_score, dt_accuracy_score, dt_recall_score, dt_f1_score]}, index = metrics_indicator)\n",
        "cm1 = confusion_matrix(y_test, dt_pred,labels = dt.classes_)\n",
        "disp1 = ConfusionMatrixDisplay(confusion_matrix = cm1, display_labels = dt.classes_)\n",
        "disp1.plot()\n",
        "plt.xticks(rotation = 90)\n",
        "plt.yticks(rotation = 0)\n",
        "plt.title('Decision Tree')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Random Forest Classifier**"
      ],
      "metadata": {
        "id": "jo26jxpqsNDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(x_train,y_train)\n",
        "rfc_pred = rfc.predict(x_test)\n",
        "\n",
        "rfc_precision_score = precision_score(y_test, rfc_pred, average = \"weighted\")\n",
        "rfc_accuracy_score = accuracy_score(y_test, rfc_pred)\n",
        "rfc_recall_score = recall_score(y_test, rfc_pred, average = \"weighted\")\n",
        "rfc_f1_score = f1_score(y_test, rfc_pred,average = \"weighted\")\n",
        "\n",
        "pmf[\"RFC\"] = pd.DataFrame({\"RFC\":[rfc_precision_score, rfc_accuracy_score,rfc_recall_score, rfc_f1_score]}, index = metrics_indicator)\n",
        "\n",
        "cm2 = confusion_matrix(y_test, rfc_pred, labels = rfc.classes_)\n",
        "disp2 = ConfusionMatrixDisplay(cm2, display_labels = rfc.classes_)\n",
        "disp2.plot()\n",
        "plt.xticks(rotation = 90)\n",
        "plt.yticks(rotation = 0)\n",
        "plt.title('Random Forest')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wxs5BaJTsRLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting KNN Model**"
      ],
      "metadata": {
        "id": "zCuLJRNWts96"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdnOBRAi8eDx"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(x_train,y_train)\n",
        "knn_pred = knn.predict(x_test)\n",
        "\n",
        "knn_precision_score = precision_score(y_test, knn_pred, average = \"weighted\")\n",
        "knn_accuracy_score = accuracy_score(y_test, knn_pred)\n",
        "knn_recall_score = recall_score(y_test, knn_pred, average = \"weighted\")\n",
        "knn_f1_score = f1_score(y_test, knn_pred, average = \"weighted\")\n",
        "\n",
        "pmf[\"KNN\"] = pd.DataFrame({\"KNN\":[knn_precision_score, knn_accuracy_score, knn_recall_score, knn_f1_score]}, index = metrics_indicator)\n",
        "\n",
        "cm3 = confusion_matrix(y_test, knn_pred, labels = knn.classes_)\n",
        "disp = ConfusionMatrixDisplay(cm3, display_labels = knn.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation = 90)\n",
        "plt.yticks(rotation = 0)\n",
        "plt.title('KNN')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Naive Bayes Model**"
      ],
      "metadata": {
        "id": "h9p9eIxQobYB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alUQ0Vid8eGB"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "nb = GaussianNB()\n",
        "nb.fit(x_train, y_train)\n",
        "nb_pred = nb.predict(x_test)\n",
        "\n",
        "nb_precision_score = precision_score(y_test, nb_pred, average = \"weighted\")\n",
        "nb_accuracy_score = accuracy_score(y_test, nb_pred)\n",
        "nb_recall_score = recall_score(y_test, nb_pred, average = \"weighted\")\n",
        "nb_f1_score = f1_score(y_test, nb_pred, average = \"weighted\")\n",
        "\n",
        "pmf[\"NB\"] = pd.DataFrame({\"NB\": [nb_precision_score, nb_accuracy_score, nb_recall_score, nb_f1_score]}, index = metrics_indicator)\n",
        "\n",
        "cm4 = confusion_matrix(y_test, nb_pred, labels = nb.classes_)\n",
        "disp = ConfusionMatrixDisplay(cm4, display_labels = nb.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation = 90)\n",
        "plt.yticks(rotation = 0)\n",
        "plt.title(\"Naive Bayes\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Gradient Boosting Model**"
      ],
      "metadata": {
        "id": "gaSXGGZSqZ7P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS9rI_yr8eIU"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(x_train,y_train)\n",
        "gb_pred = gb.predict(x_test)\n",
        "\n",
        "gb_precision_score = precision_score(y_test, gb_pred, average = \"weighted\")\n",
        "gb_accuracy_score = accuracy_score(y_test, gb_pred)\n",
        "gb_recall_score = recall_score(y_test, gb_pred, average = \"weighted\")\n",
        "gb_f1_score = f1_score(y_test, gb_pred, average = \"weighted\")\n",
        "\n",
        "pmf[\"GB\"] = pd.DataFrame({\"GB\":[gb_precision_score, gb_accuracy_score, gb_recall_score, gb_f1_score]}, index = metrics_indicator)\n",
        "\n",
        "cm5 = confusion_matrix(y_test, gb_pred, labels = gb.classes_)\n",
        "disp= ConfusionMatrixDisplay(cm5, display_labels = gb.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation = 90)\n",
        "plt.yticks(rotation = 0)\n",
        "plt.title(\"Gradient Boosting\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Ada Boost Model**"
      ],
      "metadata": {
        "id": "GZqf4OTusYCB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ3yuMYt8eKt"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "ab = AdaBoostClassifier()\n",
        "ab.fit(x_train, y_train)\n",
        "ab_pred = ab.predict(x_test)\n",
        "\n",
        "ab_precision_score = precision_score(y_test, ab_pred, average = \"weighted\")\n",
        "ab_accuracy_score = accuracy_score(y_test, ab_pred)\n",
        "ab_recall_score = recall_score(y_test, ab_pred, average = \"weighted\")\n",
        "ab_f1_score = f1_score(y_test, ab_pred, average = \"weighted\")\n",
        "\n",
        "pmf[\"AB\"] = pd.DataFrame({\"AB\":[ab_precision_score, ab_accuracy_score, ab_recall_score, ab_f1_score]}, index = metrics_indicator)\n",
        "\n",
        "cm6 = confusion_matrix(y_test, ab_pred, labels = ab.classes_)\n",
        "disp= ConfusionMatrixDisplay(cm6, display_labels = ab.classes_)\n",
        "disp.plot()\n",
        "plt.xticks(rotation = 90)\n",
        "plt.yticks(rotation = 0)\n",
        "plt.title(\"Ada Boost\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Writing Performance Metrics to File**"
      ],
      "metadata": {
        "id": "VflNVchgEvsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pmf_ad = pd.DataFrame.from_dict(pmf, orient='index', columns=['Score'])\n",
        "pmf.to_csv(\"./ Suitable_Crop_Performance_Prediction_Of_Models.csv\")"
      ],
      "metadata": {
        "id": "yQ2tAIBKD_Jc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}